{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVz_jbYc9-0",
        "outputId": "e16939d3-7dfb-40c2-859c-a64aaa2ae50f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btrczIo0S9sO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJeH9Wnjd_ow",
        "outputId": "98dc86d7-331f-4698-a954-d6657925400b"
      },
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdbc1A8UOO8j"
      },
      "source": [
        "#Importing neccessary libraries\n",
        "import nltk,re,string\n",
        "from nltk.corpus import stopwords,twitter_samples\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7jx8_bqPT6m"
      },
      "source": [
        "# Preprocessing of the tweets that is our data\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    stemmer = nltk.PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and\n",
        "                word not in string.punctuation):\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzonsw6RSEc5"
      },
      "source": [
        "#This is the most important part of the whole code \n",
        "#The reason is our feature set through which will be training our model on will build here."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnvV39HoS0Aq"
      },
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo9BKjcuamWB"
      },
      "source": [
        "#Check the above code work with an example"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Jn1Oj7cIln",
        "outputId": "b5a8e2ac-8436-4a18-da62-5e8b8f776a68"
      },
      "source": [
        "tweets=['i am happy','i am tricked','i am sad','i am tired','i am emotional']\n",
        "ys=[1,0,0,0,0]\n",
        "res=build_freqs(tweets,ys)\n",
        "print(res)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 1, ('emot', 0): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FtiDs_Pc0gD"
      },
      "source": [
        "#Select the set of positive and negative tweets\n",
        "all_positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets=twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ou5gJETd-Uq"
      },
      "source": [
        "#Split the data into two pieces,one for training and testing\n",
        "test_pos=all_positive_tweets[4000:]\n",
        "train_pos=all_positive_tweets[4000:]\n",
        "test_neg=all_negative_tweets[4000:]\n",
        "train_neg=all_negative_tweets[4000:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBam5PAwey_y"
      },
      "source": [
        "#Combine positive and negative labels\n",
        "train_x=train_pos+train_neg\n",
        "test_x=test_pos+test_neg\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vmLY5VnfKc5"
      },
      "source": [
        "#we are building our y-target variable here\n",
        "train_y=np.append(np.ones((len(train_pos),1)),np.zeros((len(train_neg),1)),axis=0)\n",
        "test_y=np.append(np.ones((len(test_pos),1)),np.zeros((len(test_neg),1)),axis=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKb4QZ0KfKf2"
      },
      "source": [
        "#Create frequency dictionary \n",
        "freqs=build_freqs(train_x,train_y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAsKmNHLfKkX",
        "outputId": "7b3a5540-8462-46fc-e582-44eb057ba53a"
      },
      "source": [
        "#check out the output\n",
        "print('type(freqs)='+str(type(freqs)))\n",
        "print('len(freqs='+str(len(freqs.keys())))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs)=<class 'dict'>\n",
            "len(freqs=4571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WleOK4WfKnt",
        "outputId": "6d29badf-7ceb-4d5e-d6a5-92eac2b1d233"
      },
      "source": [
        "#test function below\n",
        "print('This is an example of a positive tweet:\\n',train_x[22])\n",
        "print('\\nThis is an example of processed version of the tweet:\\n',process_tweet(train_x[22]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet:\n",
            " @mommydean74 @Patriotic_Me Delighted, m'dear!:)\n",
            "\n",
            "This is an example of processed version of the tweet:\n",
            " ['delight', \"m'dear\", ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5smpb2afKsE"
      },
      "source": [
        "#Unlike most ,we actually build logistic regression model from scratch\n",
        "#Logistic Regression\n",
        "#Sigmoid Function\n",
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      z: is the input (can be a scaler or an array)\n",
        "  output:\n",
        "      h:the sigmoid of z\n",
        "  \"\"\"\n",
        "  zz=np.negative(z)\n",
        "  h=1/(1 + np.exp(zz))\n",
        "  return h"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BvDCTKBfKu1"
      },
      "source": [
        "# Cost function and Gradient\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    \"\"\"\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    for i in range(0, num_iters):\n",
        "        z = np.dot(x, theta)\n",
        "        h = sigmoid(z)\n",
        "        # calculate the cost function\n",
        "        cost = -1. / m * (np.dot(y.transpose(), np.log(h)) + np.dot((1 - y).transpose(), np.log(1 - h)))\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n",
        "\n",
        "    cost = float(cost)\n",
        "    return cost, theta\n",
        "\n",
        "   \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8uVeniqfKxo"
      },
      "source": [
        "# Extracting the feature\n",
        "def extract_features(tweet,freqs):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      tweet:a list of words for one tweet\n",
        "      freqs:a dictionary corresponding to the frequencies of each tuple (word,label)\n",
        "  Output:\n",
        "      x:a feature vector of dimension (1,3)\n",
        "  \"\"\"\n",
        "\n",
        "  word_1=process_tweet(tweet)\n",
        "  x=np.zeros((1,3))\n",
        "  #Bias term is set of 1\n",
        "  x[0,0]=1\n",
        "  for word in word_1:\n",
        "    #increatment the word count for the positive label 1\n",
        "    x[0,1]+=freqs.get((word,1.0),0)\n",
        "    #increatment the word count for the negative label 0\n",
        "    x[0,2]+=freqs.get((word,0.0),0)\n",
        "\n",
        "  assert (x.shape==(1,3))\n",
        "  return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZfodV-VfK0W",
        "outputId": "e567ee1e-fbc6-4fb5-ae15-60567538698d"
      },
      "source": [
        "#test on training data \n",
        "tmp1 = extract_features(train_x[22], freqs)\n",
        "print(tmp1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1. 725.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtSlmbVcfK3I"
      },
      "source": [
        "#Try to undertand what all this 3 number mean\n",
        "#Usually we get a dataset with a lot features/columns ,here we just have text data.\n",
        "#Those three number are feature set that we have build using build_freqs and extract feature function.\n",
        "#build_freqs builds a dictionary having words as keys and the number of times they have occured in corpus as values \n",
        "#Extract features takes in sum of these values for positive and negative words that tmp1 and tmp2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYLLq338fK5k"
      },
      "source": [
        "#How these features wil be used to predict the logistic regression\n",
        "#First hypothesis is build which for our case will be h(x)=b1+b2**1 + b3**2\n",
        "#here b1=1,b2 and b3 are determined by cost and gradient function ,x1 and x2 are the positive and negative word feature set."
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEh3h9bqUUFf"
      },
      "source": [
        "#Training the model\n",
        "#collect the feature x and stack them into matrix x\n",
        "X=np.zeros((len(train_x),3))\n",
        "for i in range (len(train_x)):\n",
        "  X[i, :]=extract_features(train_x[i],freqs)\n",
        "#Training labels corresponding to x \n",
        "Y=train_y\n",
        "\n",
        "#apply gradient descent\n",
        "#these value are predefined\n",
        "J,theta=gradientDescent(X,Y,np.zeros((3,1)),1e-9,1500)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YkHyWw9VvPH"
      },
      "source": [
        "def predict_tweet(tweet,freqs,theta):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      tweet:a string \n",
        "      freqs:a dictionary corresponding to the frequencies of each tuple(word,label)\n",
        "      theta:(3,1)vector of weights\n",
        "\n",
        "  Output:\n",
        "       y_pred:the probabilities of a tweet bring positive or negative\n",
        "  \"\"\"\n",
        "  #extract the feature of the tweet and store it into x \n",
        "  x=extract_features(tweet,freqs)\n",
        "  y_pred=sigmoid(np.dot(x,theta))\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWb3G0y5VvZC"
      },
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            y_hat.append(0)\n",
        "\n",
        "    accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnKPJL_ZVvb6",
        "outputId": "4330e542-094e-407e-e71a-f88b02a8827a"
      },
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIwi3NXNaKkV"
      },
      "source": [
        "#Predict with your own tweet\n",
        "def pre(sentence):\n",
        "  yhat=predict_tweet(sentence,freqs,theta)\n",
        "  if yhat>0.5:\n",
        "    return 'Positive sentiment'\n",
        "  elif yhat==0:\n",
        "    return 'Neutral sentiment'\n",
        "  else:\n",
        "    return 'Negative sentiment'"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HGLp9ODboRJ",
        "outputId": "3f2a5115-830a-4128-cd70-a6950b774170"
      },
      "source": [
        "my_tweet='it is too bad today,boring day'\n",
        "res=pre(my_tweet)\n",
        "print(res)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD-iGM_9cUJy",
        "outputId": "9391a14f-684f-42bb-c3f8-18e13f623978"
      },
      "source": [
        "my_tweet='it is so hot today but it is perfect day for the beach party'\n",
        "res=pre(my_tweet)\n",
        "print(res)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3BS1erFceyY",
        "outputId": "707dbc97-2a9d-42fd-9ea8-2d217a2c5265"
      },
      "source": [
        "my_tweet='vishal is a bad boy'\n",
        "res=pre(my_tweet)\n",
        "print(res)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYDp2hgkdSUW",
        "outputId": "cfac9bcf-0fc5-442f-d85d-b396ff7e0435"
      },
      "source": [
        "my_tweet='vishal is a good boy'\n",
        "res=pre(my_tweet)\n",
        "print(res)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GG97Zprdpnf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}